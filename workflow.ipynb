{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StudyPal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the PDF you want to study from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "['Operating Systems (UE18CS302)\\nUnit 2\\nAronya Baksy\\nAugust 2020\\n1 Threads\\n\\x0fThreads are fundamental units of CPU execution, with each thread containing its own program\\ncounter, register set, stack and an identi\\x0cer known as the thread ID .\\n\\x0fProcesses contain single or multiple threads of execution. Multiple threads allow a process to\\nperform multiple tasks at the same time (eg: most modern desktop applications like Web Browsers,\\nWord Processors, spreadsheet programs etc. are multithreaded).\\n\\x0fProcess creation is heavy and requires large overheads (mostly from context switching) while thread\\ncreation is lightweight with minimal overheads.\\n1.1 Bene\\x0cts of Multithreaded Programming\\n\\x0fResponsiveness : As threads are independent of each other, one can run even when another one\\nis blocked (ie. waiting). This is useful in GUI applications where each thread can service one part\\nof the UI and keep response times low.\\n\\x0fResource Sharing : Processes share resources using complicated IPC mechanisms, but threads\\ncan share resources easily as they use the memory address space of the process that created them.\\n\\x0fEconomy : Thread creation has minimal memory allocation and context switching overheads\\ncompared to process creation as threads share a common memory, while processes need their own\\nmemory and context.\\n\\x0fScalability : A multi-threaded processes can run on multiple processing cores, while a single-\\nthreaded process can run only on one core (irrespective of how many cores are there in hardware).\\n1.2 Multicore Programming (Concurrency vs Parallelism)\\n\\x0fMultithreaded programming provides a mechanism for more e\\x0ecient use of these multiple comput-\\ning cores and improved concurrency\\n\\x0fIf an application has 4 threads, then on a single CPU system, the 4 threads will be interleaved but\\nat a single instant, only one thread will be executed.\\n\\x0fOn a multicore system, a separate core can be assigned to run each thread, hence the threads can\\nrun in parallel.\\n\\x0fConcurrent systems support the execution of more than one task, but only one task is executed\\nat a single instant of time.\\n\\x0fParallel systems can execute multiple tasks at the same time, meaning that at a single isntance\\nof time, there are >1 tasks running.\\n\\x0fConcurrent systems can give the illusion of being parallel with rapid and frequent switching between\\nthreads, but this is not true parallelism.\\n\\x0fNowadays, all hardware manufacturers support multiple hardware threads per core (modern Intel\\nand AMD CPUs have 2 threads per core)\\n1', \"Figure 1: Single and Multi-Threaded Architectures\\n1.2.1 Amdahl's Law\\n\\x0fAmdahl's Law states that the the speedup obtained from running a program with a fraction of\\nserial code given by SonNcomputational units (cores) is given by\\nspeedup =1\\nS+1\\x00S\\nN(1)\\n\\x0fAmdahl's law proposes that the serial portion of the program has a disproportionate e\\x0bect on any\\nperformance gain derived from adding more cores. (as n!1 ,speedup!1\\nS)\\n\\x0fSome argue that Amdahl's Law does not take into account the hardware performance enhancements\\nused in the design of contemporary multicore systems.\\n1.3 Programming Challenges\\n\\x0fIdentifying tasks that are simple and independent within the program logic.\\n\\x0fBalanced workloads on all tasks. If some tasks are less valuabe to the \\x0cnal output, then it may\\nnot be worth the e\\x0bort to run them on separate cores.\\n\\x0fData splitting to run on multiple cores\\n\\x0fData dependencies between threads. Threads must be synchronized to avoid problems with\\nmultiple threads writing to/reading from the same memory location.\\n\\x0fTesting and debugging is more complicated due to many di\\x0berent execution paths possible.\\n1.4 Types of Parallelism\\n\\x0fData-level parallelism involves distributing the data into multiple cores and each thread (running\\non a core) does the same task on this subset of the data\\n\\x0fTask-level parallelism involves di\\x0berent threads doing di\\x0berent tasks, on either the same data or\\ndi\\x0berent data.\\n\\x0fReal world applications use a hybrid of both types of parallelism.\\n2\", '2 Multithreading Models\\nThese are di\\x0berent models of establishing a relationship between user andkernel threads.\\n2.1 Many-to-One Model\\n\\x0fMultiple user threads are mapped to a single kernel thread.\\n\\x0fThread management is done by the thread library in user space, hence it is e\\x0ecient.\\n\\x0fHowever if one thread executes a blocking system call, then all the the threads must stop execution.\\n\\x0fAlso, multiple cores cannot be taken advantage of as only one kernel thread is present to access\\nthe kernel at a time, hence multiple user threads cannot run at the same time\\n\\x0feg: Green threads on Solaris and early Java versions\\n2.2 One-to-One Model\\n\\x0fOne user thread is mapped to one kernel thread\\n\\x0fGreater concurrency as another thread can be made to run when one thread makes a blocking\\nsystem call. Multiple threads can also run in parallel on multiprocessors.\\n\\x0fSince creating a user thread also means creating a corresponding kernel thread, the overheads are\\nhigh, and the OS can restrict the number of user threads being created.\\n\\x0feg: Linux, Windows family OSes\\n2.3 Many-to-Many Model\\n\\x0fMultiple user threads are multiplexed to a smaller or equal number of kernel threads.\\n\\x0fThe developer can create as many user threads as needed, and the corresponding kernel threads\\ncan run in parallel on multicore systems.\\n\\x0fIf one thread does a blocking system call, another kernel thread can be scheduled to run.\\n\\x0fIn atwo-level system , many user threads are still multiplexed to a certain number of kernel\\nthreads, but user threads can also be bound to a kernel thread. (eg: Solaris <9)\\n3 Thread Scheduling\\n\\x0fOn systems that support user-level and kernel-level threads, the kernel-level threads and not pro-\\ncesses are scheduled by the operating system.\\n\\x0fIn order to be scheduled, user threads need to be mapped to a kernel thread, although this mapping\\nmay not be direct but might be via a lightweight process (LWP).\\n3.1 Contention Scope\\n\\x0fOn many-to-one and many-to-many implementations, the thread library schedules user threads\\nonto available LWPs. This scheduling is called Process Contention Scope or PCS scheduling.\\n\\x0fPCS is called so because competition for CPU now happens between user threads scheduled on\\none process.\\n\\x0fTo decide which kernel thread should be scheduled onto the CPU, System Contention Scope\\nor SCS scheduling is used. SCS scheduling is used among all the threads in the system.\\n\\x0fPCS is done according to priority|the scheduler selects the runnable thread with the highest\\npriority to run.\\n3', '\\x0fUser-level thread priorities are set by the programmer and are not adjusted by the thread library,\\nalthough some thread libraries may allow the programmer to change the priority of a thread.\\n\\x0fPCS will typically preempt the thread currently running in favor of a higher-priority thread; how-\\never, there is no guarantee of time slicing among threads of equal priority.\\n4 Thread Libraries\\n\\x0fThread Libraries are APIs for developers to create and manage threads\\n\\x0fThey may be implemented entirely at the user level (ie. the data structures and the code are all\\nin user space) and no system calls are made.\\n\\x0fThey may also be implemented with kernel support, ie. code and data structures are in kernel\\nspace, and function calls of this API involve system calls.\\n\\x0fThe three main thread libraries in use today are pthreads , Windows Threads and the Java thread-\\ning library (for applications running on the JVM).\\n4.1 pthread code example\\nThe following is a code example that launches a separate thread to compute the sum of the \\x0crst n natural\\nnumbers.\\n#include <stdio.h>\\n#include <pthread.h>\\nvoid* runner(void* arg);\\nint sum = 0;\\nint main(int argc, char* argv[])\\n{\\npthread_attr_t attr;\\npthread_t tid;\\nint num = atoi(argv[1]);\\npthread_attr_init(&attr);\\npthread_create(&tid, &attr, runner, (void*)&num);\\npthread_join(&tid);\\nprintf(\"Sum = %d\\\\n\", sum);\\n}\\nvoid* runner(void* arg)\\n{\\nint n = *((int*)arg);\\nfor(int i=1; i<n; ++i)\\nsum += i;\\npthread_exit(NULL);\\n}\\npthread code example\\n\\x0fThe pthread attr t()function is used to initialize the pthread attr tstructure that holds the\\nthread attributes.\\n\\x0fThe pthread create() function takes in the pthread tstructure, the attributes, the pointer to\\nthe function to be run on that thread (which must have signature of void* function(void* arg) )\\nand the argument to be passed to the thread function.\\n4', '\\x0fInside the thread function, the void* is converted to intand the computation is done. At the\\nend of the computation, the pthread exit(NULL) function call signi\\x0ces the end of that thread and\\nreturn of control back to the main process.\\n\\x0fThe pthread join(&tid) function is used to make the main process wait for the child thread called\\ntidto \\x0cnish executing.\\n5 Process Synchronization\\n\\x0fProcess synchronization is required to avoid race conditions .\\n\\x0fA race condition is a situation where multiple threads or processes access the same data concur-\\nrently, and the \\x0cnal outcome is dependent on the order in which the processes/threads accessed\\nthe data.\\n\\x0fSynchronization ensures that only one process can access the data at any instant in time\\n5.1 The Critical-Section Problem\\n\\x0fIn a system consisting of nprocessesfP0; P1; P2; :::; P n\\x001g, each process has a region of code called\\nthecritical section where it performs operations like updating a global variables, updating a\\ntable or opening a \\x0cle.\\n\\x0fThe important feature of the critical section is that when one process is executing its critical section,\\nno other process is allowed to execute its critical section. That is, no two processes may execute\\ntheir critical sections at the same time.\\n\\x0fEach processes requests entry into its critical process in the entry section , executes the critical\\nsection, and exits using the exit section . All other code in the process falls under the remainder\\nsection .\\n\\x0fThe problem statement is to design a protocol that the nprocesses can use to coordinate their\\nexecution.\\n\\x0fThe solution to the critical-section problem has the following characteristics:\\n{It must ensure mutual exclusion , ie. make sure that if process Piis executing its critical\\nsection, then no other process is executing its own critical section.\\n{It must ensure progress . If no process is executing its critical section, and there are some\\nprocesses that wish to enter their critical sections, then only processes that are not executing\\ntheir remainder sections are eligible to be selected to run their critical section next, and this\\nselection cannot be postponed inde\\x0cnitely.\\n{In short, progress implies that if a process is not using the critical section, then it should not\\nstop any other process from accessing it.\\n{ Bounded waiting . Between a process requesting access to the critical section, and the\\ngranting of the request, there is a \\x0cnite number of times that all other processes can access\\ntheir critical sections.\\n{In other words, the waiting time for a process between request for access and actual access\\nmust not be in\\x0cnite.\\n{Read this link for clear explanation.\\n\\x0fNo assumptions are made about the relative speed of the processes, but it is assumed that all\\nprocesses are executing at non-zero speed.\\n\\x0fIn kernel mode, there are many scopes for race conditions (eg: two processes simultaneously opening\\na \\x0cle may cause a race condition in the \\x0cle table maintained by the OS).\\n\\x0fWith preemptive kernels , a process can be preempted when it is running in kernel mode. This\\nsort of kernel can have race conditions, and is more prone to race condition in SMP (Symmetric\\nMultiProcessing) systems, as more than one kernel process may run on di\\x0berent processors.\\n5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "# creating a pdf reader object\n",
    "reader = PdfReader('OS_notes.pdf') # replace with your pdf file name\n",
    " \n",
    "# printing number of pages in pdf file\n",
    "print(len(reader.pages))\n",
    "\n",
    "pdfText = []\n",
    " \n",
    "# getting a specific page from the pdf file\n",
    "for i in range(5):\n",
    "    page = reader.pages[i]\n",
    "    pdfText.append(page.extract_text())\n",
    "\n",
    "print(pdfText)\n",
    "\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling Open AI API to get summary , questions and falshcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the first page of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Threads are independent units of CPU execution that contain their own program counter, register set, stack, and thread ID. Processes can contain single or multiple threads of execution, allowing multiple tasks to be performed simultaneously. Multithreaded programming has several benefits, including improved responsiveness, easy resource sharing, and lower memory allocation and context switching overheads compared to process creation. Multicore programming provides a mechanism for more efficient use of multiple computing cores and improved concurrency, allowing multiple tasks to be executed at the same time. However, concurrent systems can give the illusion of being parallel with rapid and frequent switching between threads, but this is not true parallelism. Modern CPUs have multiple hardware threads per core.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1680348081,\n",
      "  \"id\": \"chatcmpl-70TaLNgNjNkniMTOOFBNudYGszfUE\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 138,\n",
      "    \"prompt_tokens\": 545,\n",
      "    \"total_tokens\": 683\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model = \"gpt-3.5-turbo\",\n",
    "  messages = [{\"role\": \"user\", \"content\": \"Summaries the following\" + pdfText[0]}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads are independent units of CPU execution that contain their own program counter, register set, stack, and thread ID. Processes can contain single or multiple threads of execution, allowing multiple tasks to be performed simultaneously. Multithreaded programming has several benefits, including improved responsiveness, easy resource sharing, and lower memory allocation and context switching overheads compared to process creation. Multicore programming provides a mechanism for more efficient use of multiple computing cores and improved concurrency, allowing multiple tasks to be executed at the same time. However, concurrent systems can give the illusion of being parallel with rapid and frequent switching between threads, but this is not true parallelism. Modern CPUs have multiple hardware threads per core.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get questions based on the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". What are threads?\n",
      "a. Independent programs\n",
      "b. Fundamental units of CPU execution\n",
      "c. Processes containing multiple tasks\n",
      "d. Units of memory allocation\n",
      "\n",
      "2. What is the benefit of multithreaded programming in GUI applications?\n",
      "a. Improved resource sharing\n",
      "b. Minimal memory allocation\n",
      "c. Reduced response times\n",
      "d. Scalability\n",
      "\n",
      "3. How do threads share resources compared to processes?\n",
      "a. Using complicated IPC mechanisms\n",
      "b. Easily using the memory address space of the process\n",
      "c. Through a common memory pool\n",
      "d. By creating separate memory segments for each thread\n",
      "\n",
      "4. What is the advantage of thread creation over process creation?\n",
      "a. Reduced overheads\n",
      "b. Improved resource allocation\n",
      "c. Increased scalability\n",
      "d. Better memory optimization\n",
      "\n",
      "5. How many threads can be executed at a single instant on a single CPU system?\n",
      "a. Only one\n",
      "b. Two\n",
      "c. Three\n",
      "d. Four\n",
      "\n",
      "6. What is the difference between concurrency and parallelism?\n",
      "a. Concurrency supports the execution of more tasks, while parallelism can execute multiple tasks simultaneously.\n",
      "b. Concurrency can execute multiple tasks simultaneously, while parallelism supports the execution of more tasks.\n",
      "c. Concurrent systems switch between threads rapidly, giving the illusion of being parallel, while parallel systems truly execute multiple tasks simultaneously.\n",
      "d. Parallel systems are more efficient than concurrent systems in executing multiple tasks.\n",
      "\n",
      "7. Can a single-threaded process run on multiple processing cores?\n",
      "a. Yes\n",
      "b. No\n",
      "\n",
      "8. What is the benefit of multicore systems in multisystem programming?\n",
      "a. Improved concurrency\n",
      "b. Improved resource sharing\n",
      "c. Minimal memory allocation\n",
      "d. Reduced response times\n",
      "\n",
      "9. What is true parallelism in computing?\n",
      "a. Rapid switching between tasks\n",
      "b. The execution of multiple tasks at a single instant of time\n",
      "c. Improved resource allocation\n",
      "d. Increases in scalability\n",
      "\n",
      "10. How many hardware threads per core do modern Intel and AMD CPUs have?\n",
      "a. 1\n",
      "b. 2\n",
      "c. 3\n",
      "d. 4\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model = \"gpt-3.5-turbo\",\n",
    "  messages = [{\"role\": \"user\", \"content\": \"give me 10 multiple choice question from the following\" + pdfText[0]}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10 flashcards based on the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".3 Thread States\n",
      "\u000fThreads can be in one of the following states : \n",
      "\u000b New State : The thread has just been created, but has not yet started execution\n",
      "\u000b Runnable State : The thread is ready to run and waiting for a turn on the CPU\n",
      "\u000b Running State : The thread is currently being executed\n",
      "\u000b Blocked State : The thread is unable to run because it is waiting for some event to occur (eg: I/O\n",
      "completions, a lock to be released etc.)\n",
      "\u000b Terminated State : The thread has finished execution and is no longer active \n",
      "2 Scheduling\n",
      "2.1 Types of Scheduling\n",
      "\u000fScheduling is an important function of the OS, enabling multiple threads or processes to run concur-\n",
      "rently on a single processing unit.\n",
      "\u000fScheduling algorithms can be broadly classified into two types - preemptive and non-preemptive. \n",
      "\u000fIn Preemptive Scheduling, the OS can interrupt the execution of a running process/thread to give\n",
      "processing time to another process/thread. \n",
      "\u000fIn Non-Preemptive Scheduling, the running process/thread continues until it terminates, blocks or\n",
      "voluntarily yields the CPU.\n",
      "2.2 Process Scheduling\n",
      "\u000fProcess scheduling is the act of selecting a process from the queue of waiting processes to execute\n",
      "on the CPU.\n",
      "\u000fScheduling decisions can be based on priority, time quantum, execution time, etc.\n",
      "2.3 Thread Scheduling\n",
      "\u000fThread scheduling involves selecting a thread from the running queue to execute on the CPU.\n",
      "\u000fScheduling decisions can be based on thread priority, thread dependency, thread affinity, etc.\n",
      "3 Mutual Exclusion\n",
      "3.1 Race Conditions and Deadlocks\n",
      "\u000fRace conditions occur when multiple threads access shared data and the outcome of the execution\n",
      "is dependent on the order of the execution of the threads.\n",
      "\u000fDeadlocks occur when two or more threads or processes are waiting for each other to release the\n",
      "shared resources leading to a situation where none of them can proceed.\n",
      "3.2 The Critical Section Problem\n",
      "\u000fThe Critical Section is a segment of code in which multiple threads might compete for access to a\n",
      "shared resource.\n",
      "\u000fThe Critical Section Problem is the problem of avoiding race-conditions and deadlocks in the critical\n",
      "section.\n",
      "\u000fSolutions include the use of semaphores, mutexes, monitors, etc.\n",
      "4 Memory Management\n",
      "4.1 Memory Management Goals\n",
      "\u000fMemory management is the process of allocating and deallocating memory blocks to different pro-\n",
      "grams and threads.\n",
      "\u000fGoals include minimizing fragmentation, maximizing utilization, reducing overheads and ensuring\n",
      "data security.\n",
      "4.2 Memory Allocation Schemes\n",
      "\u000fMemory allocation schemes include single block allocation (for single threaded applications), buddy\n",
      "systems (for multithreaded applications) and Slab allocation (for kernel memory allocation).\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model = \"gpt-3.5-turbo\",\n",
    "  messages = [{\"role\": \"user\", \"content\": \"give me 10 flashcards to revise all the topics\" + pdfText[0]}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content) # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
